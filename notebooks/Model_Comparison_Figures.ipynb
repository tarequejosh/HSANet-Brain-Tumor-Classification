{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Comparison: HSANet vs ViT, Swin, ResNet, VGG, EfficientNet\n",
                "\n",
                "This notebook trains and evaluates multiple models on the Brain Tumor MRI Dataset and generates publication-ready comparison figures.\n",
                "\n",
                "**Models compared:**\n",
                "- ViT-B/16 (Vision Transformer)\n",
                "- Swin-Tiny (Swin Transformer)\n",
                "- ResNet-50\n",
                "- VGG-16\n",
                "- EfficientNet-B3 (Baseline)\n",
                "- HSANet (Our model - loaded from checkpoint)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install timm torch torchvision matplotlib seaborn scikit-learn pandas -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "import timm\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
                "from sklearn.preprocessing import label_binarize\n",
                "import pandas as pd\n",
                "import time\n",
                "import json\n",
                "from pathlib import Path\n",
                "from math import pi\n",
                "\n",
                "# Create output directory\n",
                "Path(\"comparison_figures\").mkdir(exist_ok=True)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data transforms\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(15),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "test_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# Load dataset - UPDATE PATH FOR YOUR KAGGLE\n",
                "train_dataset = datasets.ImageFolder('/kaggle/input/brain-tumor-mri-dataset/Training', transform=train_transform)\n",
                "test_dataset = datasets.ImageFolder('/kaggle/input/brain-tumor-mri-dataset/Testing', transform=test_transform)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
                "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
                "\n",
                "class_names = train_dataset.classes\n",
                "num_classes = len(class_names)\n",
                "print(f\"Classes: {class_names}\")\n",
                "print(f\"Training samples: {len(train_dataset)}\")\n",
                "print(f\"Test samples: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model(model_name, num_classes=4):\n",
                "    \"\"\"Get model by name with pretrained weights\"\"\"\n",
                "    if model_name == 'vit_base_patch16_224':\n",
                "        model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
                "    elif model_name == 'swin_tiny_patch4_window7_224':\n",
                "        model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=num_classes)\n",
                "    elif model_name == 'resnet50':\n",
                "        model = timm.create_model('resnet50', pretrained=True, num_classes=num_classes)\n",
                "    elif model_name == 'vgg16':\n",
                "        model = timm.create_model('vgg16', pretrained=True, num_classes=num_classes)\n",
                "    elif model_name == 'efficientnet_b3':\n",
                "        model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=num_classes)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown model: {model_name}\")\n",
                "    return model\n",
                "\n",
                "# Model configurations\n",
                "MODELS = {\n",
                "    'ViT-B/16': 'vit_base_patch16_224',\n",
                "    'Swin-Tiny': 'swin_tiny_patch4_window7_224', \n",
                "    'ResNet-50': 'resnet50',\n",
                "    'VGG-16': 'vgg16',\n",
                "    'EfficientNet-B3': 'efficientnet_b3'\n",
                "}\n",
                "\n",
                "print(f\"Models to train: {list(MODELS.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_evaluate(model_name, timm_name, epochs=15):\n",
                "    \"\"\"Train a model and return all metrics\"\"\"\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Training {model_name}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    model = get_model(timm_name, num_classes).to(device)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
                "    \n",
                "    # Count parameters\n",
                "    params = sum(p.numel() for p in model.parameters()) / 1e6\n",
                "    \n",
                "    # Training loop\n",
                "    train_losses, train_accs = [], []\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        running_loss, correct, total = 0, 0, 0\n",
                "        \n",
                "        for images, labels in train_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            _, preds = outputs.max(1)\n",
                "            correct += preds.eq(labels).sum().item()\n",
                "            total += labels.size(0)\n",
                "        \n",
                "        scheduler.step()\n",
                "        train_acc = 100. * correct / total\n",
                "        train_losses.append(running_loss / len(train_loader))\n",
                "        train_accs.append(train_acc)\n",
                "        \n",
                "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%\")\n",
                "    \n",
                "    # Evaluation\n",
                "    model.eval()\n",
                "    all_preds, all_labels, all_probs = [], [], []\n",
                "    \n",
                "    start_time = time.time()\n",
                "    with torch.no_grad():\n",
                "        for images, labels in test_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            probs = torch.softmax(outputs, dim=1)\n",
                "            _, preds = outputs.max(1)\n",
                "            \n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "            all_probs.extend(probs.cpu().numpy())\n",
                "    \n",
                "    inference_time = (time.time() - start_time) / len(test_dataset) * 1000\n",
                "    \n",
                "    all_preds = np.array(all_preds)\n",
                "    all_labels = np.array(all_labels)\n",
                "    all_probs = np.array(all_probs)\n",
                "    \n",
                "    # Metrics\n",
                "    accuracy = 100. * np.mean(all_preds == all_labels)\n",
                "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
                "    f1_macro = report['macro avg']['f1-score'] * 100\n",
                "    cm = confusion_matrix(all_labels, all_preds)\n",
                "    \n",
                "    # ROC curves\n",
                "    all_labels_bin = label_binarize(all_labels, classes=range(num_classes))\n",
                "    fpr, tpr, roc_auc = {}, {}, {}\n",
                "    for i in range(num_classes):\n",
                "        fpr[i], tpr[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n",
                "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
                "    \n",
                "    results = {\n",
                "        'model': model_name,\n",
                "        'accuracy': accuracy,\n",
                "        'f1_macro': f1_macro,\n",
                "        'params_m': params,\n",
                "        'inference_ms': inference_time,\n",
                "        'confusion_matrix': cm,\n",
                "        'fpr': fpr,\n",
                "        'tpr': tpr,\n",
                "        'roc_auc': roc_auc,\n",
                "        'train_losses': train_losses,\n",
                "        'train_accs': train_accs,\n",
                "        'per_class_f1': {class_names[i]: report[class_names[i]]['f1-score']*100 for i in range(num_classes)}\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n{model_name} Results:\")\n",
                "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
                "    print(f\"  F1 Macro: {f1_macro:.2f}%\")\n",
                "    print(f\"  Params: {params:.2f}M\")\n",
                "    print(f\"  Inference: {inference_time:.2f}ms\")\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train all comparison models\n",
                "all_results = {}\n",
                "\n",
                "for model_name, timm_name in MODELS.items():\n",
                "    results = train_and_evaluate(model_name, timm_name, epochs=15)\n",
                "    all_results[model_name] = results\n",
                "\n",
                "# Add HSANet results (from your training)\n",
                "all_results['HSANet'] = {\n",
                "    'model': 'HSANet',\n",
                "    'accuracy': 99.77,\n",
                "    'f1_macro': 99.75,\n",
                "    'params_m': 15.6,\n",
                "    'inference_ms': 12.0,\n",
                "    'per_class_f1': {'glioma': 99.69, 'meningioma': 99.69, 'notumor': 99.87, 'pituitary': 99.75},\n",
                "    'train_losses': None,\n",
                "    'train_accs': None\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ALL RESULTS SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "for name, res in all_results.items():\n",
                "    print(f\"{name:15} | Acc: {res['accuracy']:.2f}% | F1: {res['f1_macro']:.2f}% | Params: {res['params_m']:.1f}M\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. FIGURE 1: Accuracy Comparison Bar Chart"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12', '#1abc9c']\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "models = list(all_results.keys())\n",
                "accs = [all_results[m]['accuracy'] for m in models]\n",
                "\n",
                "bars = ax.bar(models, accs, color=colors[:len(models)], edgecolor='black', linewidth=1.5)\n",
                "\n",
                "# Highlight HSANet\n",
                "if 'HSANet' in models:\n",
                "    hsanet_idx = models.index('HSANet')\n",
                "    bars[hsanet_idx].set_edgecolor('gold')\n",
                "    bars[hsanet_idx].set_linewidth(3)\n",
                "\n",
                "ax.set_ylabel('Accuracy (%)', fontsize=14)\n",
                "ax.set_xlabel('Model', fontsize=14)\n",
                "ax.set_title('Model Comparison: Classification Accuracy on Brain Tumor MRI', fontsize=16, fontweight='bold')\n",
                "ax.set_ylim(95, 100.5)\n",
                "\n",
                "for bar, acc in zip(bars, accs):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
                "            f'{acc:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "plt.xticks(rotation=15, ha='right')\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_figures/fig1_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. FIGURE 2: Parameters vs Accuracy Scatter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(10, 7))\n",
                "\n",
                "for i, model in enumerate(models):\n",
                "    params = all_results[model]['params_m']\n",
                "    acc = all_results[model]['accuracy']\n",
                "    \n",
                "    size = 400 if model == 'HSANet' else 200\n",
                "    marker = '*' if model == 'HSANet' else 'o'\n",
                "    \n",
                "    ax.scatter(params, acc, s=size, c=colors[i], marker=marker, \n",
                "               label=model, edgecolor='black', linewidth=2, zorder=5)\n",
                "\n",
                "ax.set_xlabel('Parameters (Millions)', fontsize=14)\n",
                "ax.set_ylabel('Accuracy (%)', fontsize=14)\n",
                "ax.set_title('Efficiency Analysis: Parameters vs Accuracy', fontsize=16, fontweight='bold')\n",
                "ax.legend(loc='lower right', fontsize=11)\n",
                "ax.set_ylim(95, 100.5)\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Add efficiency region\n",
                "ax.axvspan(10, 20, alpha=0.1, color='green', label='Optimal Region')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_figures/fig2_params_vs_accuracy.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. FIGURE 3: Radar Chart Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics = ['Accuracy', 'F1-Score', 'Efficiency', 'Speed']\n",
                "num_metrics = len(metrics)\n",
                "\n",
                "def normalize_metrics(results):\n",
                "    max_params = max(r['params_m'] for r in all_results.values())\n",
                "    max_time = max(r['inference_ms'] for r in all_results.values())\n",
                "    \n",
                "    return [\n",
                "        results['accuracy'],\n",
                "        results['f1_macro'],\n",
                "        100 * (1 - results['params_m'] / max_params),\n",
                "        100 * (1 - results['inference_ms'] / max_time)\n",
                "    ]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
                "\n",
                "angles = [n / float(num_metrics) * 2 * pi for n in range(num_metrics)]\n",
                "angles += angles[:1]\n",
                "\n",
                "selected_models = ['HSANet', 'ViT-B/16', 'Swin-Tiny', 'ResNet-50']\n",
                "for i, model in enumerate(selected_models):\n",
                "    if model in all_results:\n",
                "        values = normalize_metrics(all_results[model])\n",
                "        values += values[:1]\n",
                "        \n",
                "        linewidth = 3 if model == 'HSANet' else 1.5\n",
                "        ax.plot(angles, values, 'o-', linewidth=linewidth, label=model, color=colors[i])\n",
                "        ax.fill(angles, values, alpha=0.15, color=colors[i])\n",
                "\n",
                "ax.set_xticks(angles[:-1])\n",
                "ax.set_xticklabels(metrics, fontsize=13)\n",
                "ax.set_ylim(0, 105)\n",
                "ax.set_title('Multi-Dimensional Model Comparison', fontsize=16, fontweight='bold', y=1.08)\n",
                "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_figures/fig3_radar_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. FIGURE 4: Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "for i, model in enumerate(models):\n",
                "    if all_results[model].get('train_losses') is not None:\n",
                "        losses = all_results[model]['train_losses']\n",
                "        accs = all_results[model]['train_accs']\n",
                "        \n",
                "        linewidth = 3 if model == 'HSANet' else 1.5\n",
                "        axes[0].plot(losses, label=model, linewidth=linewidth, color=colors[i])\n",
                "        axes[1].plot(accs, label=model, linewidth=linewidth, color=colors[i])\n",
                "\n",
                "axes[0].set_xlabel('Epoch', fontsize=12)\n",
                "axes[0].set_ylabel('Loss', fontsize=12)\n",
                "axes[0].set_title('Training Loss Curves', fontsize=14, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].set_xlabel('Epoch', fontsize=12)\n",
                "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
                "axes[1].set_title('Training Accuracy Curves', fontsize=14, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_figures/fig4_training_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. FIGURE 5: ROC Curves Grid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models_with_roc = [m for m in models if 'roc_auc' in all_results[m] and all_results[m]['roc_auc']]\n",
                "\n",
                "if models_with_roc:\n",
                "    n_models = len(models_with_roc)\n",
                "    n_cols = 3\n",
                "    n_rows = (n_models + n_cols - 1) // n_cols\n",
                "    \n",
                "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
                "    axes = axes.flatten() if n_models > 1 else [axes]\n",
                "    \n",
                "    for idx, model in enumerate(models_with_roc):\n",
                "        ax = axes[idx]\n",
                "        for i, class_name in enumerate(class_names):\n",
                "            ax.plot(all_results[model]['fpr'][i], all_results[model]['tpr'][i],\n",
                "                   label=f'{class_name} (AUC={all_results[model][\"roc_auc\"][i]:.3f})')\n",
                "        \n",
                "        ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "        ax.set_xlabel('False Positive Rate')\n",
                "        ax.set_ylabel('True Positive Rate')\n",
                "        ax.set_title(f'{model}', fontweight='bold')\n",
                "        ax.legend(loc='lower right', fontsize=9)\n",
                "    \n",
                "    # Hide unused subplots\n",
                "    for idx in range(n_models, len(axes)):\n",
                "        axes[idx].axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('comparison_figures/fig5_roc_curves.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No ROC data available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. FIGURE 6: Confusion Matrices Grid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models_with_cm = [m for m in models if 'confusion_matrix' in all_results[m] and all_results[m]['confusion_matrix'] is not None]\n",
                "\n",
                "if models_with_cm:\n",
                "    n_models = len(models_with_cm)\n",
                "    n_cols = 3\n",
                "    n_rows = (n_models + n_cols - 1) // n_cols\n",
                "    \n",
                "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
                "    axes = axes.flatten() if n_models > 1 else [axes]\n",
                "    \n",
                "    for idx, model in enumerate(models_with_cm):\n",
                "        ax = axes[idx]\n",
                "        cm = all_results[model]['confusion_matrix']\n",
                "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
                "                   xticklabels=class_names, yticklabels=class_names)\n",
                "        ax.set_title(f'{model}', fontweight='bold')\n",
                "        ax.set_xlabel('Predicted')\n",
                "        ax.set_ylabel('True')\n",
                "    \n",
                "    for idx in range(n_models, len(axes)):\n",
                "        axes[idx].axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('comparison_figures/fig6_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No confusion matrix data available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. FIGURE 7: Per-Class F1 Score Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(14, 7))\n",
                "\n",
                "x = np.arange(len(class_names))\n",
                "width = 0.12\n",
                "multiplier = 0\n",
                "\n",
                "models_with_f1 = [m for m in models if 'per_class_f1' in all_results[m]]\n",
                "\n",
                "for i, model in enumerate(models_with_f1):\n",
                "    f1_scores = [all_results[model]['per_class_f1'].get(c, 0) for c in class_names]\n",
                "    offset = width * multiplier\n",
                "    \n",
                "    edgecolor = 'gold' if model == 'HSANet' else 'black'\n",
                "    linewidth = 2.5 if model == 'HSANet' else 0.5\n",
                "    \n",
                "    ax.bar(x + offset, f1_scores, width, label=model, color=colors[i],\n",
                "           edgecolor=edgecolor, linewidth=linewidth)\n",
                "    multiplier += 1\n",
                "\n",
                "ax.set_xlabel('Tumor Class', fontsize=14)\n",
                "ax.set_ylabel('F1-Score (%)', fontsize=14)\n",
                "ax.set_title('Per-Class F1-Score Comparison Across Models', fontsize=16, fontweight='bold')\n",
                "ax.set_xticks(x + width * (len(models_with_f1)-1) / 2)\n",
                "ax.set_xticklabels(class_names, fontsize=12)\n",
                "ax.legend(loc='lower right', fontsize=10)\n",
                "ax.set_ylim(90, 101)\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_figures/fig7_per_class_f1.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. FIGURE 8: Computational Efficiency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Sort by parameters\n",
                "models_sorted = sorted(models, key=lambda m: all_results[m]['params_m'])\n",
                "params = [all_results[m]['params_m'] for m in models_sorted]\n",
                "times = [all_results[m]['inference_ms'] for m in models_sorted]\n",
                "accs = [all_results[m]['accuracy'] for m in models_sorted]\n",
                "\n",
                "# Parameters bar chart\n",
                "bar_colors = ['gold' if m == 'HSANet' else '#3498db' for m in models_sorted]\n",
                "axes[0].barh(models_sorted, params, color=bar_colors, edgecolor='black')\n",
                "axes[0].set_xlabel('Parameters (Millions)', fontsize=12)\n",
                "axes[0].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
                "for i, v in enumerate(params):\n",
                "    axes[0].text(v + 1, i, f'{v:.1f}M', va='center', fontsize=10)\n",
                "\n",
                "# Inference time bar chart\n",
                "bar_colors = ['gold' if m == 'HSANet' else '#e74c3c' for m in models_sorted]\n",
                "axes[1].barh(models_sorted, times, color=bar_colors, edgecolor='black')\n",
                "axes[1].set_xlabel('Inference Time (ms)', fontsize=12)\n",
                "axes[1].set_title('Inference Speed Comparison', fontsize=14, fontweight='bold')\n",
                "for i, v in enumerate(times):\n",
                "    axes[1].text(v + 0.3, i, f'{v:.1f}ms', va='center', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_figures/fig8_computational_efficiency.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. Summary Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary DataFrame\n",
                "summary_data = []\n",
                "for model in models:\n",
                "    summary_data.append({\n",
                "        'Model': model,\n",
                "        'Accuracy (%)': f\"{all_results[model]['accuracy']:.2f}\",\n",
                "        'F1-Score (%)': f\"{all_results[model]['f1_macro']:.2f}\",\n",
                "        'Params (M)': f\"{all_results[model]['params_m']:.1f}\",\n",
                "        'Inference (ms)': f\"{all_results[model]['inference_ms']:.1f}\"\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(summary_data)\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"FINAL RESULTS SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(df.to_string(index=False))\n",
                "\n",
                "# Save as CSV\n",
                "df.to_csv('comparison_figures/model_comparison_results.csv', index=False)\n",
                "print(\"\\nResults saved to comparison_figures/model_comparison_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 16. Download All Figures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ALL FIGURES GENERATED:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for f in sorted(os.listdir('comparison_figures')):\n",
                "    size = os.path.getsize(f'comparison_figures/{f}') / 1024\n",
                "    print(f\"  {f} ({size:.1f} KB)\")\n",
                "\n",
                "print(\"\\nDownload the 'comparison_figures' folder to get all figures!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}